{
  "config_name": "Windows + Ollama (NO GPU)",
  "description": "Configuration for Windows users using Ollama without GPU",
  "environment": {
    "platform": "windows",
    "model_type": "llava",
    "requires_gpu": false,
    "requires_docker": false,
    "requires_wsl": false
  },
  "model_settings": {
    "model_type": "llava",
    "ollama_model_name": "llava:13b",
    "api_key_required": false,
    "ollama_url": "http://localhost:11434/api/generate"
  },
  "processing_settings": {
    "max_workers": 2,
    "timeout": 120,
    "optimize_images": true,
    "max_dimension": 1024,
    "quality": 85,
    "min_dimension": 200
  },
  "system_monitoring": {
    "check_lightroom": true,
    "max_memory_usage": 0.8,
    "max_cpu_usage": 0.7
  },
  "output_settings": {
    "generate_xmp": false,
    "modify_exif": true,
    "enable_gallery_critique": false,
    "critique_threshold": 5,
    "output_file": "ollama_analysis_results.csv",
    "log_file": "ollama_analyzer.log"
  },
  "installation": {
    "python_version": "3.9+",
    "required_packages": [
      "Pillow",
      "piexif",
      "psutil",
      "flask",
      "flask-socketio"
    ],
    "optional_packages": [],
    "setup_instructions": [
      "1. Install Python 3.9 or higher",
      "2. Create virtual environment: python -m venv venv",
      "3. Activate environment: venv\\Scripts\\activate",
      "4. Install packages: pip install -r requirements_ollama.txt",
      "5. Install Ollama: https://ollama.com/downloads",
      "6. Run: python scripts/unified_analyzer.py --model llava [directory]"
    ]
  },
  "features": {
    "supports_web_gui": true,
    "supports_batch_processing": true,
    "supports_xmp_sidecar": true,
    "supports_exif_metadata": true,
    "supports_progress_tracking": true,
    "supports_gallery_critique": true
  },
  "performance": {
    "expected_speed": "Moderate",
    "memory_usage": "Moderate",
    "internet_required": false
  }
}
