#!/usr/bin/env python3
"""
Installation Verification Script for AI Image Analyzer
Checks all dependencies and system requirements
"""

import sys
import subprocess
import importlib
from pathlib import Path

def check_python_version():
    """Check Python version compatibility"""
    print("üêç Checking Python version...")
    version = sys.version_info
    print(f"   Python {version.major}.{version.minor}.{version.micro}")
    
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        print("   ‚ùå Python 3.8+ required")
        return False
    else:
        print("   ‚úÖ Python version OK")
        return True

def check_package(package_name, import_name=None, optional=False):
    """Check if a Python package is available"""
    if import_name is None:
        import_name = package_name
        
    try:
        importlib.import_module(import_name)
        print(f"   ‚úÖ {package_name}")
        return True
    except ImportError:
        if optional:
            print(f"   ‚ö†Ô∏è  {package_name} (optional)")
            return True
        else:
            print(f"   ‚ùå {package_name} - Missing")
            return False

def check_ollama_connection():
    """Check Ollama installation and connection"""
    print("ü¶ô Checking Ollama...")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m['name'] for m in models]
            
            print("   ‚úÖ Ollama is running")
            print(f"   üì¶ Available models: {', '.join(model_names) if model_names else 'None'}")
            
            # Check for recommended models
            recommended_models = ['llava:13b', 'llava:7b', 'llava']
            found_model = False
            
            for model in recommended_models:
                if any(model in name for name in model_names):
                    print(f"   ‚úÖ Found recommended model: {model}")
                    found_model = True
                    break
            
            if not found_model:
                print("   ‚ö†Ô∏è  No LLaVA models found. Run: ollama pull llava:13b")
            
            return True
        else:
            print(f"   ‚ùå Ollama responded with HTTP {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("   ‚ùå Ollama not running or not installed")
        print("   üí° Install Ollama from https://ollama.com")
        return False
    except Exception as e:
        print(f"   ‚ùå Error connecting to Ollama: {e}")
        return False

def check_gpu_support():
    """Check GPU availability"""
    print("üéÆ Checking GPU support...")
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            print(f"   ‚úÖ CUDA GPU available: {gpu_name}")
            print(f"   üíæ GPU Memory: {gpu_memory:.1f} GB")
            
            if gpu_memory >= 8:
                print("   üöÄ Excellent GPU for LLaVA 13B")
            elif gpu_memory >= 4:
                print("   ‚ö° Good GPU for LLaVA 7B")
            else:
                print("   ‚ö†Ô∏è  Limited GPU memory - consider CPU processing")
                
            return True
        else:
            print("   ‚ö†Ô∏è  No CUDA GPU detected - will use CPU")
            return True
            
    except ImportError:
        print("   ‚ùå PyTorch not installed")
        return False
    except Exception as e:
        print(f"   ‚ùå Error checking GPU: {e}")
        return False

def main():
    """Run all verification checks"""
    print("üîç AI Image Analyzer - Installation Verification")
    print("=" * 50)
    
    all_good = True
    
    # Core Python version check
    if not check_python_version():
        all_good = False
    
    print("\nüì¶ Checking Python packages...")
    
    # Required packages
    required_packages = [
        ("Pillow", "PIL"),
        ("requests", "requests"),
        ("google-generativeai", "google.generativeai"),
        ("PyTorch", "torch"),
        ("torchvision", "torchvision"),
        ("PyIQA", "pyiqa"),
        ("piexif", "piexif"),
        ("PyExifTool", "exiftool"),
        ("psutil", "psutil"),
    ]
    
    for package_name, import_name in required_packages:
        if not check_package(package_name, import_name):
            all_good = False
    
    # Built-in packages (should always be available)
    print("\nüèóÔ∏è Checking built-in packages...")
    builtin_packages = [
        ("tkinter", "tkinter"),
        ("json", "json"),
        ("threading", "threading"),
        ("pathlib", "pathlib"),
    ]
    
    for package_name, import_name in builtin_packages:
        check_package(package_name, import_name)
    
    print()
    
    # Ollama check
    if not check_ollama_connection():
        print("   üí° Ollama is recommended for local processing")
    
    print()
    
    # GPU check
    check_gpu_support()
    
    print("\n" + "=" * 50)
    
    if all_good:
        print("üéâ Installation verification completed successfully!")
        print("üöÄ You can now run: python main.py")
    else:
        print("‚ùå Some issues found. Please install missing packages:")
        print("   pip install -r requirements.txt")
        
    return all_good

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)